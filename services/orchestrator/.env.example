# LLM Configuration (REQUIRED)
# Base URL for the OpenAI-compatible API endpoint
LLM_BASE_URL=https://api.openai.com/v1

# API key for authentication
LLM_API_KEY=your-api-key-here

# Model name (optional, default: gpt-4)
# Examples: gpt-4, gpt-4-turbo, gemini-2.5-flash, claude-3-opus
LLM_MODEL_NAME=gpt-4

# Temperature for response generation (optional, default: 0.7)
# Range: 0.0 (deterministic) to 1.0 (creative)
LLM_TEMPERATURE=0.7

# Maximum tokens in response (optional, default: 2000)
LLM_MAX_TOKENS=2000

# MCP Configuration (optional)
# Path to MCP configuration file
# If not set, defaults to ./mcp_config.yaml or /config/mcp_config.yaml (in Docker)
# MCP_CONFIG_PATH=/path/to/custom/mcp_config.yaml

# Server Configuration (optional)
# HOST=0.0.0.0
# PORT=8000
