---
Epic: 5
Story: 2
Title: Implement LangChain LLM Client
Status: Done
---

### Story

As a Developer, I want to implement a LangChain-based LLM client that supports OpenAI-compatible endpoints, so that the system can work with multiple LLM providers and leverage LangChain's structured output capabilities.

### Acceptance Criteria

1. A new LangChain LLM client module is created at `src/app/services/langchain_llm_client.py`
2. The client supports OpenAI-compatible API endpoints via environment variables (LLM_BASE_URL, LLM_API_KEY, LLM_MODEL_NAME)
3. The client implements structured output support using Pydantic models (ExtractedEntities, AnalysisResult)
4. The client is integrated into the FastAPI dependency injection system
5. The old LLM client remains temporarily for comparison purposes

### Dev Notes

#### Module Location
- **File Path:** `src/app/services/langchain_llm_client.py`
[Source: `.kiro/specs/orchestrator-refactoring/tasks.md` Task 2.1]

#### Configuration
The client should be configured via environment variables:
- **LLM_BASE_URL**: Base URL for the LLM API (e.g., "https://api.openai.com/v1" or Gemini-compatible endpoint)
- **LLM_API_KEY**: API key for authentication
- **LLM_MODEL_NAME**: Model name (e.g., "gpt-4" or "gemini-2.5-flash")
- **LLM_TEMPERATURE**: Temperature setting (default: 0.7)
- **LLM_MAX_TOKENS**: Max tokens (default: 2000)
[Source: `.kiro/specs/orchestrator-refactoring/design.md` Section 2]

#### Structured Output Models
```python
class ExtractedEntities(BaseModel):
    pod_name: Optional[str]
    namespace: str = "default"
    error_summary: str
    error_type: Optional[str]  # e.g., "crash", "oom", "network"

class AnalysisResult(BaseModel):
    root_cause: str
    confidence: Literal["high", "medium", "low"]
    reasoning: str
    recommendations: List[str]
```
[Source: `.kiro/specs/orchestrator-refactoring/design.md` Section 2]

#### Key Methods
- `__init__(config: LLMConfig)`: Initialize the client with configuration
- `extract_entities(description: str) -> ExtractedEntities`: Extract structured entities using LangChain's structured output
- `analyze_evidence(evidence: Dict[str, Any], knowledge_graph: Dict) -> AnalysisResult`: Analyze collected evidence
[Source: `.kiro/specs/orchestrator-refactoring/design.md` Section 2]

#### LangChain Integration
Use `langchain.chat_models.init_chat_model` for model initialization to support multiple providers:
```python
from langchain.chat_models import init_chat_model

model = init_chat_model(
    model=os.getenv("LLM_MODEL", "gpt-4"),
    base_url=os.getenv("LLM_BASE_URL"),
    api_key=os.getenv("LLM_API_KEY")
)
```
[Source: `.kiro/specs/orchestrator-refactoring/design.md` Section 1]

### Tasks / Subtasks

- [x] **(AC: 1)** **Create LangChain LLM Client Module:**
  - Create file `src/app/services/langchain_llm_client.py`
  - Implement `LLMConfig` class for configuration
  - Implement `LangChainLLMClient` class

- [x] **(AC: 2)** **Implement Configuration Loading:**
  - Add environment variable loading for LLM_BASE_URL, LLM_API_KEY, LLM_MODEL_NAME
  - Add optional parameters for temperature and max_tokens
  - Use `init_chat_model` for model initialization

- [x] **(AC: 3)** **Create Structured Output Models:**
  - Define `ExtractedEntities` Pydantic model
  - Define `AnalysisResult` Pydantic model
  - Implement `extract_entities()` method using structured output
  - Implement `analyze_evidence()` method using structured output

- [x] **(AC: 4)** **Add Dependency Injection:**
  - Create `get_langchain_llm_client()` dependency function
  - Update FastAPI app to make client available
  - Keep old LLM client temporarily for comparison

- [x] **Write Unit Tests:**
  - Test entity extraction with mock LLM responses
  - Test structured output parsing
  - Test error handling
  - Test configuration loading

- [x] **Create Git Commit:**
  - Commit message: "feat: implement LangChain LLM client with OpenAI compatibility"
  - Include all LangChain client implementation files

### Change Log

| Date | Version | Description | Author |
| --- | --- | --- | --- |
| 2025-11-14 | 1.0 | Initial draft | Yi (Dev) |

### Dev Agent Record

#### Agent Model Used
_TBD_

#### Debug Log References
_TBD_

#### Completion Notes List
-

#### File List
-

### QA Results

- **Status:** Not yet reviewed
- **Date:**
- **Reviewed by:**
- **Summary:**
- **Issues Found:**
- **Recommendations:**
