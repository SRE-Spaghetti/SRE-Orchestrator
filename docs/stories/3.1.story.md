---
Epic: 3
Story: 1
Title: LLM Integration for Intent Extraction
Status: Draft
---

### Story

As the Orchestrator, I want to send the description from a new incident to an LLM, so that I can extract key entities like the pod name, namespace, and the nature of the problem.

### Acceptance Criteria

1.  The orchestrator service is configured with the credentials to connect to a cloud-based LLM API (e.g., OpenAI, Google AI).
2.  When a new incident is created, the orchestrator constructs a specific prompt for the LLM, asking it to extract the pod name, namespace, and a summary of the error from the incident's `description` field.
3.  The orchestrator sends the prompt to the LLM and parses the structured response (e.g., JSON) to get the extracted entities.
4.  The extracted entities are stored with the incident data.
5.  The hard-coded pod name from Story 2.4 is replaced with the pod name extracted by the LLM.

### Dev Notes

#### External API Integration
- **API:** Cloud LLM API (Provider TBD) [Source: `architecture/external-apis.md`]
- **Authentication:** The API key for the LLM service must be managed via environment variables for local development and Kubernetes Secrets in production. It must never be hardcoded. [Source: `architecture/security.md`]
- **Client:** A dedicated LLM client should be created to handle the API communication.

#### Project Structure
- **LLM Client:** Create a new client service in the Orchestrator at `services/orchestrator/app/services/llm_client.py`. [Source: `architecture/source-tree.md`]
- **Logic Integration:** The incident creation workflow in `services/orchestrator/app/core/` should be updated to use this new client.

#### Data Models
- **Incident Model:** The `Incident` Pydantic model in the Orchestrator should be updated to include the `extracted_entities` field, which will be a dictionary. [Source: `architecture/data-models.md`]

### Tasks / Subtasks

1.  **(AC: 1)** **Create LLM Client Service:**
    - In `services/orchestrator/app/services/`, create `llm_client.py`.
    - Choose a Python client library for the target LLM provider (e.g., `openai`, `google-generativeai`). Add it to `pyproject.toml`.
    - Create a client class that is initialized with the API key from an environment variable.
    - Implement a method `extract_entities(description: str)` that will perform the prompt and parsing logic.
    - Provide this client to other services via FastAPI's dependency injection.
2.  **(AC: 2)** **Develop Prompt:**
    - Inside the `extract_entities` method, create a system prompt that instructs the LLM to act as an SRE assistant.
    - The user prompt should contain the incident description and ask for a JSON object containing `pod_name`, `namespace`, and `error_summary`.
3.  **(AC: 3, 4, 5)** **Update Incident Creation Workflow:**
    - Modify the incident creation logic in `services/orchestrator/app/core/`.
    - After creating the incident, call the `llm_client.extract_entities` method with the incident's description.
    - Store the returned JSON object in the new `extracted_entities` field of the incident object.
    - Update the call to the `k8s_agent_client` to use the `pod_name` and `namespace` from the `extracted_entities` instead of the hard-coded parsing logic from Story 2.4.
4.  **Update Data Model and Endpoint:**
    - Add the `extracted_entities` field to the `Incident` Pydantic model in `services/orchestrator/app/models/`.
    - Ensure the `GET /api/v1/incidents/{id}` endpoint now includes this new field in its response.
5.  **Write Unit Tests:**
    - In `services/orchestrator/tests/unit/`, write unit tests for the new `LLMClient`, mocking the external API call.
    - Test that the client constructs the correct prompt and parses the LLM's JSON response correctly.
    - Update the unit tests for the incident creation workflow to verify that the `LLMClient` is called and the `extracted_entities` are stored.

### QA Results

- **Status:** Not yet reviewed
- **Date:**
- **Reviewed by:**
- **Summary:**
- **Issues Found:**
- **Recommendations:**
